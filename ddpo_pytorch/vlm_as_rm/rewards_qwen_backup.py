from PIL import Image
import io
import numpy as np
import torch
import requests
import json
import base64
import re
import yaml
import sys
import os
import os.path as osp
import random

import concurrent.futures

from torchvision import transforms
from torch.utils.data import DataLoader
from datasets import load_dataset

import cv2
from decord import VideoReader
import pandas as pd
from tqdm import tqdm


sys.path.append("/video/wangjiahao08/workspace/data_engine")
sys.path.append("/video/wangjiahao08/workspace/data_engine/gpt4_utils")
#
from openai import OpenAI



random.seed(42)

bizs = ['liangjiajun_939ed227_gpt-4o-2024-08-06']

def evaluate_QwenVL2_7B():
    
    with open("ddpo_pytorch/prompt_relative.yaml") as f:
        data = yaml.safe_load(f)

    FullPmt = """
        Conduct image comparison which is generated by a prompt through these phases:

        1. **Checklist Generation**:
        - Identify critical comparison dimensions 
        - Assign weights (sum=100%) based on task importance

        2. **Structured Comparison**:
        - Tabular format: "Dimension | Image1 | Image2 | Preference | Weight"
        - Preference codes:  
            - 1 = Image1 superior
            - -1 = Image2 superior
            - 0 = Equivalent

        3. **Deep Analysis**:
        - Detailed reasoning for each dimension comparison
        - Explain judgment rationale and relative importance
        - Consider contextual factors and edge cases

        4. **Holistic Verdict**:
        - Synthesize analysis beyond numerical scores
        - Final declaration format: "Final Verdict: image1/tie/image2"

        Output MUST follow EXACTLY:
        ====BEGIN CHECKLIST====
        1. Dimension1: Weight%
        2. Dimension2: Weight%
        ...
        ====END CHECKLIST====

        ====BEGIN COMPARISON====
        Dimension | Image1 | Image2 | Preference | Weight
        Dimension1 | desc | desc | code | X%
        ...
        ====END COMPARISON====

        ====BEGIN ANALYSIS====
        [Free-form analytical reasoning demonstrating critical evaluation of 
        comparison results and their contextual implications]
        ====END ANALYSIS====

        ====BEGIN VERDICT====
        Final Verdict: Image1/Tie/Image2
        ====END VERDICT====
    Now, the prompt of generated images is {locPrompt}. The provided image is structured such that the **left part represents Image 1** and the **right part represents Image 2**. 
    """
    
    CKLST_PROMPT = data["cklst-prompt"]
    EVAL_PROMPT = data["eval-prompt"]

    COMP_MAPPING = {
        "1 >> 2": 2,
        "1 > 2": 1,
        "1 = 2": 0,
        "1 < 2": -1,
        "1 << 2": -2,
        "2 >> 1": -2,
        "2 > 1": -1,
        "2 = 1": 0,
        "2 < 1": 1,
        "2 << 1": 2
    }

    def encodeAsPIL(imagelist, target_size=512,  as_base64=False):
        images = []
        total_width = 0
        max_height = 0

        for image_data in imagelist:
            img = image_data.convert("RGB")
            original_width, original_height = img.size

            new_height = target_size
            new_width = int((target_size / original_height) * original_width)
            img = img.resize((new_width, new_height))  # Resize the image
            images.append(img)

            total_width += new_width
            max_height = max(max_height, new_height)

        # Create a blank combined image
        combined_image = Image.new('RGB', (total_width + 5 * (len(imagelist) - 1), max_height))

        x_offset = 0
        for img in images:
            combined_image.paste(img, (x_offset, 0))
            x_offset += img.width + 5

            # If as_base64 is True, return Base64-encoded string
        if as_base64:
            img_byte_arr = io.BytesIO()
            combined_image.save(img_byte_arr, format='JPEG')
            img_byte_arr.seek(0)  # Move to the beginning of the BytesIO stream
            img_base64 = base64.b64encode(img_byte_arr.read()).decode('utf-8')
            return img_base64

        # Otherwise, return the PIL Image object
        return combined_image
    
    def validate_format(text):
        required_blocks = {
            'CHECKLIST': r'(\d+\.\s.+?:\s\d+%)',
            'COMPARISON': r'([^|]+\|[^|]+\|[^|]+\|(-1|0|1)\|\s?\d+%)',
            'ANALYSIS': r'\S+',  # At least some non-whitespace content
            'VERDICT': r'Final Verdict:\s*(Image1|Tie|Image2)'
        }
        
        try:
            # Block existence check
            for block in required_blocks:
                if not re.search(f'====BEGIN {block}====.*====END {block}====', text, re.DOTALL):
                    return False

            # Checklist validation
            checklist = re.findall(r'\d+\.\s(.+?):\s(\d+)%', 
                                re.search(r'====BEGIN CHECKLIST====(.*?)====END CHECKLIST====', text, re.DOTALL).group(1))
            if sum(int(w) for _,w in checklist) != 100 or len(checklist) < 2:
                return False

            # Comparison table validation
            comparison = re.search(r'====BEGIN COMPARISON====(.*?)====END COMPARISON====', text, re.DOTALL).group(1)
            lines = [line.strip() for line in comparison.split('\n') if line.strip()]
            if len(lines) < 2:
                return False
                
            # Verify comparison line format
            pattern = re.compile(r'^[^|]+\|[^|]+\|[^|]+\|(-1|0|1)\|\s?\d+%$')
            if not all(pattern.match(line) for line in lines[1:]):
                return False

            # Analysis content check
            analysis_content = re.search(r'====BEGIN ANALYSIS====(.*?)====END ANALYSIS====', text, re.DOTALL).group(1).strip()
            if len(analysis_content) < 50:  # Minimum analysis length
                return False

            return True
            
        except Exception as e:
            print(f"Validation error: {str(e)}")
            return False

    def parse_output(text):
        result = {
            'checklist': [],
            'comparison': [],
            'analysis': '',
            'verdict': None,
            'verdict_code': None
        }
        
        try:
            # Parse checklist
            checklist_match = re.search(r'====BEGIN CHECKLIST====(.*?)====END CHECKLIST====', text, re.DOTALL)
            if checklist_match:
                for m in re.finditer(r'\d+\.\s(.+?):\s(\d+)%', checklist_match.group(1)):
                    result['checklist'].append({
                        'dimension': m[1].strip(),
                        'weight': int(m[2])
                    })

            # Parse comparison table
            comparison_match = re.search(r'====BEGIN COMPARISON====(.*?)====END COMPARISON====', text, re.DOTALL)
            if comparison_match:
                lines = [line.strip() for line in comparison_match.group(1).split('\n') if line.strip()]
                header = lines[0].lower().split('|')
                for line in lines[1:]:
                    parts = [p.strip() for p in line.split('|')]
                    if len(parts) == 5:
                        result['comparison'].append({
                            'dimension': parts[0],
                            'image1': parts[1],
                            'image2': parts[2],
                            'preference': int(parts[3]),
                            'weight': int(parts[4].replace('%',''))
                        })

            # Parse analysis
            analysis_match = re.search(r'====BEGIN ANALYSIS====(.*?)====END ANALYSIS====', text, re.DOTALL)
            if analysis_match:
                result['analysis'] = analysis_match.group(1).strip()
            else:
                result['analysis'] = ""

            # Parse final verdict
            verdict_match = re.search(r'Final Verdict:\s*(Image1|Tie|Image2)', text)
            if verdict_match:
                verdict = verdict_match.group(1) 
                result['verdict'] = verdict
                if verdict == 'Image1':
                    result['verdict_code'] = 1
                elif verdict == 'Image2':
                    result['verdict_code'] = -1
                else:  # Tie
                    result['verdict_code'] = 0
            else:
                result['verdict'] = ""
                result['verdict_code'] = 0

            return result
            
        except Exception as e:
            print(f"Parsing error: {str(e)}")
            return None

    def verify_decision_integrity(parsed_data):
        """Ensure verdict aligns with analysis content"""
        if not parsed_data:
            return False
        
        # 2. Verify weight dimensions match
        checklist_dims = {item['dimension'] for item in parsed_data['checklist']}
        comparison_dims = {item['dimension'] for item in parsed_data['comparison']}
        if checklist_dims != comparison_dims:
            return False
            
        # 3. Validate analysis depth
        analysis_words = len(parsed_data['analysis'].split())
        if analysis_words < 100:  # Require substantial analysis
            return False
            
        return True

    def _fn(batch_data, Error=None, DEBUG=False, ANALYSIS=None, RECORD=False, cklth=3, toolbox=None):
        """
        打分
        """
        if toolbox is None:
            raise ValueError("pipe in Qwen")
        
        inputs = batch_data.pop["invs"]
        inputs = inputs.to("cuda")
        
        # generate方法
        pad_token_id = processor.tokenizer.pad_token_id
        generated_outputs = model.generate(
            **inputs,
            max_new_tokens=4096,
            pad_token_id=pad_token_id,  # 确保填充符正确
            return_dict_in_generate=True  # 获取完整生成信息
        )
        # 获取生成的 token 和 logits
        generated_sequences = generated_outputs.sequences
        attention_mask = (generated_sequences != pad_token_id).long()

        generated_output = model(
            input_ids=generated_sequences,
            attention_mask=attention_mask,  # 关键：传入mask
            return_dict=True,
            output_hidden_states=True,
            use_cache=False
        )

        # generated_output = model(input_ids=generated_sequence, return_dict=True, output_hidden_states=True,use_cache=False)

        logits = generated_output["logits"]
        # 获取输入的长度（用于裁剪）
        input_length = inputs.input_ids.shape[1]

        # 裁剪 logits，仅保留生成部分的 logits (跳过输入部分的 logits)
        generated_logits = logits[:, :, :]  # (batch_size, generated_length, vocab_size)
        # 计算 logp (每个生成 token 的对数概率)
        logp_list = []
        for i, output_ids in enumerate(generated_sequence[:, input_length:]):
            # 获取每个生成 token 的 logits
            token_logits = generated_logits[i]
            # 对 logits 应用 softmax 转换为概率分布
            probs = torch.softmax(token_logits, dim=-1)
            # 提取生成 token 的概率
            token_probs = probs[range(len(output_ids)), output_ids]
            # 计算对数概率
            logp = torch.log(token_probs)
            logp_list.append(logp)
        # 将 logp 转化为张量 (batch_size, generated_length)
        logp_tensor = torch.stack(logp_list, dim=0)
        total_logp = logp_tensor.mean(dim=1) 

        # 裁剪生成的 token，只保留生成部分
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids 
            in zip(inputs.input_ids, generated_sequence)
        ]
        
        # 解码生成的文本
        ans = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        breakpoint()

        

        fmt = validate_format(ans)

        res = parse_output(ans)

        RES_DICT = {
            "fomat correctness": fmt,
            "model chose": res["verdict_code"],
            "lth":len(ans),
            "logp":total_logp,
        }


        return RES_DICT
        
        
        
        # for principle, comparison in zip(principles, comparison_levels):
        #     try:
        #         Eval_Dict[principle.strip("* ")] = COMP_MAPPING[comparison.strip("* ")]
        #     except Exception:
        #         NumberOfWrongCompRES += 1
        #         WrongCompRES.append(comparison.strip("* "))

        # final_score = 0

        # for eva, score in Eval_Dict.items():
        #     final_score += score
        
        # isNULLANS = False
        # if len(Eval_Dict)==0:
        #     isNULLANS = True

        # if not isNULLANS:
        #     posANS, negANS = sum(k > 0 for k in Eval_Dict.values()), sum(k < 0 for k in Eval_Dict.values())
        #     posRATE, negRATE = posANS/len(Eval_Dict), negANS/len(Eval_Dict)
        # else:
        #     posANS, negANS, posRATE, negRATE = 0,0,0,0
        
        # if not principles:
        #     Error.write("\nRESPOND\n"+ EVAL)
        #     Error.flush()
        # elif not Eval_Dict:
        #     Error.write("\nRESPOND\n"+ EVAL)
        #     Error.flush()

        # ERROR_DICT = {
        #     "NumberOfWrongCompRES":NumberOfWrongCompRES,
        #     "WrongCompRES": WrongCompRES,
        #     "FIRST RESPOND TIME DELAY":cnt_1,
        #     "SECOND RESPOND TIME DELAY":cnt_2,
        #     "Is Null Answer":isNULLANS
        # }

        # ANS_DICT = {
        #     "Positive Rate":posRATE,
        #     "Negative Rate":negRATE,
        #     "Null Rate": 1 - posRATE - negRATE,
        #     "TOTAL": len(Eval_Dict),
        #     "Lenth":lth,
        # }

        # if DEBUG:
        #     print("[Score]")
        #     print(final_score)
        #     print("[DICT]")
        #     print(Eval_Dict)

        # if RECORD:
        #     ANALYSIS.write(ans2)
        #     ANALYSIS.flush()

        return final_score, ERROR_DICT, ANS_DICT
    
    return _fn


def evaluate_QwenVL2_7B_BL():
    
    with open("ddpo_pytorch/prompt_relative.yaml") as f:
        data = yaml.safe_load(f)

    FullPmt ="""
        Conduct a detailed image comparison based on the provided prompt {locPrompt}. The generated image is structured such that the **left part represents Image 1** and the **right part represents Image 2**. Follow the analysis through the following phases:

        Checklist Generation
        1. Identify Critical Dimensions:
        - Enumerate essential dimensions for comparison.
        - Assign importance to these dimensions

        2. Output Format:  
        - Clearly label each dimension.  
        - Provide a description and importance for each dimension.

        Detailed Comparison
        For each dimension identified in the checklist:

        - Description: Briefly describe the dimension's importance in the context of the task.  
        - Image 1 Analysis: Evaluate Image 1 based on this dimension.  
        - Image 2 Analysis: Evaluate Image 2 based on this dimension.  
        - Preference: Declare a preference for Image 1, Image 2, or a Tie, supported by reasoning.  
        - Importance: State the importance assigned to this dimension.


        Deep Analysis
        - Critical Evaluation:
        - Provide detailed reasoning for judgments made in the structured comparison phase.
        - Explain the rationale behind preferences, considering contextual factors and potential edge cases.
        - Highlight trade-offs or particularly noteworthy aspects of each image.



        Holistic Verdict
        - Synthesis:
        - Go beyond numerical scores to synthesize a holistic conclusion based on the analysis.
        - Provide an overall judgment, considering the relative importance of dimensions and contextual factors.

        - Final Declaration:  
        Final Verdict: Image 1 / Tie / Image 2  

        **Required Output Format**
        The response **must strictly adhere to the following structure**. Any deviation will render the response invalid:

        Checklist Generation
        Dimension 1: [Description of Dimension]  
        Importance: [1/2/3]

        Dimension 2: [Description of Dimension]  
        Importance: [1/2/3]

        ...

        Detailed Comparison
        Dimension 1: [Description of Dimension]  
        - Image 1: [Evaluation of Image 1 for this dimension]  
        - Image 2: [Evaluation of Image 2 for this dimension]  
        - Preference: Image 1 / Image 2 / Tie  
        - Importance: [1/2/3]

        Dimension 2: [Description of Dimension]  
        - Image 1: [Evaluation of Image 1 for this dimension]  
        - Image 2: [Evaluation of Image 2 for this dimension]  
        - Preference: Image 1 / Image 2 / Tie  
        - Importance: [1/2/3]

        ...

        Deep Analysis
        [Provide in-depth reasoning and contextual analysis here.]

        Holistic Verdict
        [Summarize the overall assessment here.]  
        Final Verdict: Image 1 / Tie / Image 2
    """

    def parse_output(text):
        result = {
            "final_verdict": None,  # 1 (Image 1), 0 (Tie), -1 (Image 2)
            "checklist_count": 0,  # Number of checklist dimensions
            "is_valid_format": False,  # If all required sections exist
            "analysis_length": 0,  # Total character count of analysis sections (Deep Analysis + Holistic Verdict)
            "total_length": len(text)  # Total character count of the entire text
        }
        
        # 1. Extract Final Verdict
        final_verdict_match = re.search(r"Final Verdict:\s*(Image 1|Tie|Image 2|Image1|Image2|Image1/Tie/Image2)", text, re.IGNORECASE)
        if final_verdict_match:
            verdict = final_verdict_match.group(1).strip().lower()
            if verdict in ["image 1", "image1"]:
                result["final_verdict"] = 1
            elif verdict in ["tie","image1/tie/image2"]:
                result["final_verdict"] = 0
            elif verdict in ["image 2", "image2"]:
                result["final_verdict"] = -1
        
        # 2. Count Checklist Dimensions (based only on Checklist Generation section)
        checklist_section_match = re.search(r"Checklist Generation(.*?)(?:Detailed Comparison|Deep Analysis|Holistic Verdict)", text, re.IGNORECASE | re.DOTALL)
        if checklist_section_match:
            checklist_section = checklist_section_match.group(1)
            checklist_matches = re.findall(r"Dimension\s*\d+:", checklist_section, re.IGNORECASE)
            result["checklist_count"] = len(checklist_matches)
        
        # 3. Check Format Validity
        # Required sections: Checklist Generation, Detailed Comparison, Deep Analysis, Holistic Verdict
        required_sections = ["Checklist Generation", "Detailed Comparison", "Deep Analysis", "Holistic Verdict"]
        section_found = all(re.search(rf"{section}", text, re.IGNORECASE) for section in required_sections)
        result["is_valid_format"] = section_found
        
        # 4. Extract Total Analysis Length (Deep Analysis + Holistic Verdict)
        analysis_section_match = re.search(r"Deep Analysis(.*?)(?:$)", text, re.IGNORECASE | re.DOTALL)
        if analysis_section_match:
            analysis_content = analysis_section_match.group(1).strip()
            result["analysis_length"] = len(analysis_content)
        
        return result
            
    def _fn(inputs,toolbox=None,accelerator=None):
        
        if toolbox is None:
            raise ValueError("pipe in Qwen")
        model, processor = toolbox

        invs = inputs.pop("invs")
        inputs = inputs.to("cuda")
        
        # generate方法
        pad_token_id = processor.tokenizer.pad_token_id
        #print(inputs.keys())
        generated_outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            pad_token_id=pad_token_id,  # 确保填充符正确
            #return_dict_in_generate=False,  # 获取完整生成信息
            #use_cache=False,
        )

        #print(f"[ok] g1")
        # 获取生成的 token 和 logits
        generated_sequences = generated_outputs.sequences
        attention_mask = (generated_sequences != pad_token_id).long()

        # generated_output = model(
        #     input_ids=generated_sequences,
        #     attention_mask=attention_mask,  # 关键：传入mask
        #     return_dict=True,
        #     output_hidden_states=True,
        #     use_cache=False
        # )
        #print(f"[ok] b1")
        #print(torch.cuda.memory_summary())
        # generated_output = model(input_ids=generated_sequence, return_dict=True, output_hidden_states=True,use_cache=False)

        # logits = generated_output["logits"]
        # 获取输入的长度（用于裁剪）
        input_length = inputs.input_ids.shape[1]

        # 裁剪 logits，仅保留生成部分的 logits (跳过输入部分的 logits)
        # generated_logits = logits  # (batch_size, generated_length, vocab_size)
        #print("[Batch size]",len(logits))
        # 计算 logp (每个生成 token 的对数概率)
        # logp_list = []
        # for i, output_ids in enumerate(generated_sequences[:, input_length:]):
        #     # 获取每个生成 token 的 logits
        #     token_logits = generated_logits[i]
        #     # 对 logits 应用 softmax 转换为概率分布
        #     probs = torch.softmax(token_logits, dim=-1)
        #     # 提取生成 token 的概率
        #     token_probs = probs[range(len(output_ids)), output_ids]
        #     # 计算对数概率
        #     logp = torch.log(token_probs)
        #     logp_list.append(logp)
        # # 将 logp 转化为张量 (batch_size, generated_length)
        # logp_tensor = torch.stack(logp_list, dim=0)
        # total_logp = logp_tensor.mean(dim=1) 

        # 裁剪生成的 token，只保留生成部分
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids 
            in zip(inputs.input_ids, generated_sequences)
        ]
        
        # 解码生成的文本
        ans = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        #breakpoint()
        retInfo = []
        rewards = []
        fmts = []
        chz = []
        lth = []
        rzlths = []
        ckptcnts = []
        for txt,inv in zip(ans,invs):
            # ANALYSIS.write(txt)
            # ANALYSIS.flush()

            resDict = parse_output(txt)

            fmt = resDict["is_valid_format"]
            chiz = resDict["final_verdict"]
            rzlths.append(resDict["analysis_length"])
            ckptcnts.append(resDict["checklist_count"])



            if not fmt:
                curr_reward = -0.5
            elif chiz is None:
                curr_reward = 0.5
            else:
                curr_reward =  int(fmt) + int( (1-2*inv) == chiz)*3 + int( chiz == 0 ) - 1.5
            
            rewards.append(curr_reward)
            fmts.append(int(fmt))
            chz.append(int( (1-2*inv) == chiz)+0.5*(chiz==0))
            lth.append(len(txt))

        retInfo = {
            "fomat correctness": sum(fmts) / len(fmts) if fmts else 0,  # 防止列表为空
            "choose correctness": sum(chz) / len(chz) if chz else 0,
            "avg lth": sum(lth) / len(lth) if lth else 0,
            "avg reward": sum(rewards) / len(rewards) if rewards else 0,
            "avg reasoning": sum(rzlths) / len(rzlths) if rzlths else 0,
            #"avg check point count": sum(ckptcnts) / len(ckptcnts) if ckptcnts else 0,

        }

        # rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(total_logp.device)

        # with torch.no_grad():
        #     with model.disable_adapter():
        #         ref_generated_output = model(
        #             input_ids=generated_sequences,
        #             attention_mask=attention_mask,  # 关键：传入mask
        #             return_dict=True,
        #             output_hidden_states=True,
        #             use_cache=False
        #         )
        #         #ref_logits = ref_output.logits[:, input_length:]
        # #print(f"[ok] b2")
        # #breakpoint()
        # #print(torch.cuda.memory_summary())

        # ref_generated_logits = ref_generated_output["logits"]
        # ref_logp_list = []
        # for i, output_ids in enumerate(generated_sequences[:, input_length:]):
        #     # 获取每个生成 token 的 logits
        #     ref_token_logits = ref_generated_logits[i]
        #     # 对 logits 应用 softmax 转换为概率分布
        #     ref_probs = torch.softmax(ref_token_logits, dim=-1)
        #     # 提取生成 token 的概率
        #     ref_token_probs = ref_probs[range(len(output_ids)), output_ids]
        #     # 计算对数概率
        #     ref_logp = torch.log(ref_token_probs)
        #     ref_logp_list.append(ref_logp)
        # # 将 logp 转化为张量 (batch_size, generated_length)
        # ref_logp_tensor = torch.stack(ref_logp_list, dim=0)
        # ref_total_logp = ref_logp_tensor.mean(dim=1)

        #breakpoint()

        # with torch.no_grad():
        #     ref_output = ref_model(inputs.input_ids, attention_mask=inputs.attention_mask, return_dict=True)
        #     ref_logits = ref_output.logits[:, input_length:]
        # ref_probs = torch.softmax(ref_logits, dim=-1)
        # probs = torch.softmax(generated_logits, dim=-1)
        # PPO超参数
        # clip_epsilon = 0.2  # 策略裁剪范围
        # entropy_coef = 0.01  # 熵正则化系数
        # value_loss_coef = 0.5  # 值函数损失系数

        # # 计算策略比率 (ratios)
        # ratios = torch.exp(total_logp - ref_total_logp)  # r_t = exp(logp - ref_logp)

        # # 使用 accelerator.gather() 收集所有卡上的 rewards_tensor
        # all_rewards_tensor = accelerator.gather(rewards_tensor)

        # # 在主进程上计算全局均值和标准差（跨 8 个样本）
        # global_mean = all_rewards_tensor.mean()
        # global_std = all_rewards_tensor.std()

        # # 将 rewards 标准化为全局均值和标准差
        # rewards_tensor = (rewards_tensor - global_mean) / (global_std + 1e-8)  # 避免除以0

        # # 裁剪策略比率
        # clipped_ratios = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon)

        # # 计算 PPO 损失（基于当前卡上的 2 个样本）
        # ppo_loss = torch.min(ratios * rewards_tensor, clipped_ratios * rewards_tensor).mean()

        # # 计算熵正则化项
        # probs = torch.softmax(generated_logits, dim=-1)  # 当前策略的概率分布
        # entropy_loss = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()  # 熵的负值

        # # 计算值函数损失 (这里使用 total_logp 作为值函数的估计)
        # # value_loss = value_loss_coef * (total_logp - rewards_tensor).pow(2).mean()

        # # 总损失（仍然基于当前卡上的 2 个样本）
        # loss = -(ppo_loss + entropy_coef * entropy_loss)
        return None, retInfo



        # with tempfile.TemporaryDirectory() as tmpdir:
            
            #     for i, pil in enumerate(images):
            #         pil = pil.resize((256, 256))
            #         pil.save(os.path.join(tmpdir, f"{i}.jpg"))

            #     wandb.log(
            #         {
            #             "images": [
            #                 wandb.Image(
            #                     os.path.join(tmpdir, f"{i}.jpg"),
            #                     caption=f"{prompt:.100} | {((-1)**i)*reward:.2f}",
            #                 )
            #                 for i in range(2)  # 仅记录两个图像
            #             ],
            #         },
            #         commit=False,
            #     )
            # wandb.log(
            #     {
            #         "correct": int(flag) + (reward == 0) / 2,
            #         "Total inference Lenth": lth,
            #     },
            #     commit=False,
            # )
        
        
        
        # for principle, comparison in zip(principles, comparison_levels):
        #     try:
        #         Eval_Dict[principle.strip("* ")] = COMP_MAPPING[comparison.strip("* ")]
        #     except Exception:
        #         NumberOfWrongCompRES += 1
        #         WrongCompRES.append(comparison.strip("* "))

        # final_score = 0

        # for eva, score in Eval_Dict.items():
        #     final_score += score
        
        # isNULLANS = False
        # if len(Eval_Dict)==0:
        #     isNULLANS = True

        # if not isNULLANS:
        #     posANS, negANS = sum(k > 0 for k in Eval_Dict.values()), sum(k < 0 for k in Eval_Dict.values())
        #     posRATE, negRATE = posANS/len(Eval_Dict), negANS/len(Eval_Dict)
        # else:
        #     posANS, negANS, posRATE, negRATE = 0,0,0,0
        
        # if not principles:
        #     Error.write("\nRESPOND\n"+ EVAL)
        #     Error.flush()
        # elif not Eval_Dict:
        #     Error.write("\nRESPOND\n"+ EVAL)
        #     Error.flush()

        # ERROR_DICT = {
        #     "NumberOfWrongCompRES":NumberOfWrongCompRES,
        #     "WrongCompRES": WrongCompRES,
        #     "FIRST RESPOND TIME DELAY":cnt_1,
        #     "SECOND RESPOND TIME DELAY":cnt_2,
        #     "Is Null Answer":isNULLANS
        # }

        # ANS_DICT = {
        #     "Positive Rate":posRATE,
        #     "Negative Rate":negRATE,
        #     "Null Rate": 1 - posRATE - negRATE,
        #     "TOTAL": len(Eval_Dict),
        #     "Lenth":lth,
        # }

        # if DEBUG:
        #     print("[Score]")
        #     print(final_score)
        #     print("[DICT]")
        #     print(Eval_Dict)

        # if RECORD:
        #     ANALYSIS.write(ans2)
        #     ANALYSIS.flush()

        return final_score, ERROR_DICT, ANS_DICT
    
    return _fn



if __name__ == "__main__":
    idx = 0
    biz = bizs[idx % len(bizs)]
    client = GPT4o_Service(biz)

    dataset_name = "/m2v_intern/liujie/research/huggingface/dataset/imagereward/fidelity_rating_dataset"
    dataset = load_dataset(dataset_name, split="train", num_proc=2)
    dataset_length = len(dataset)
    index_list = list(range(0, dataset_length))
    #print(dataset_length)
    random.shuffle(index_list)
    #print(index_list)
    process( dataset[ index_list[0] ] )

    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        results = list(tqdm(executor.map(process, [ dataset[_] for _ in  index_list ])))

    
    breakpoint()



    for batch in dataloader:
        image1 = batch["jpg_0"]
        image2 = batch["jpg_1"]
        breakpoint()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:
        image1 = batch["jpg_0"]
        image2 = batch["jpg_1"]
        breakpoint()
        results = list(tqdm(executor.map(process, [ 0 for _ in range(8192)])))



    for _ in range(5):  # 随机抽取 200 条数据
        sample = random.choice(dataset)
        image1 = sample["jpg_0"]
        image2 = sample["jpg_1"]
        prompt = sample["caption"]
        fn = evaluate_promptImagePair_GPT4()
        fn(image1,image2, prompt)



def evaluate_QwenVL2_7B():
    
    with open("ddpo_pytorch/prompt_relative.yaml") as f:
        data = yaml.safe_load(f)

    FullPmt = """
        Conduct image comparison which is generated by a prompt through these phases:

        1. **Checklist Generation**:
        - Identify critical comparison dimensions 
        - Assign importances (sum=100%) based on task importance

        2. **Detailed Comparison**:
        - Tabular format: "Dimension | Image1 | Image2 | Preference | Importance"
        - Preference codes:  
            - 1 = Image1 superior
            - -1 = Image2 superior
            - 0 = Equivalent

        3. **Deep Analysis**:
        - Detailed reasoning for each dimension comparison
        - Explain judgment rationale and relative importance
        - Consider contextual factors and edge cases

        4. **Holistic Verdict**:
        - Synthesize analysis beyond numerical scores
        - Final declaration format: "Final Verdict: image1/tie/image2"

        Output MUST follow EXACTLY:
        ====BEGIN CHECKLIST====
        1. Dimension1: Importance%
        2. Dimension2: Importance%
        ...
        ====END CHECKLIST====

        ====BEGIN COMPARISON====
        Dimension | Image1 | Image2 | Preference | Importance
        Dimension1 | desc | desc | code | X%
        ...
        ====END COMPARISON====

        ====BEGIN ANALYSIS====
        [Free-form analytical reasoning demonstrating critical evaluation of 
        comparison results and their contextual implications]
        ====END ANALYSIS====

        ====BEGIN VERDICT====
        Final Verdict: Image1/Tie/Image2
        ====END VERDICT====
    Now, the prompt of generated images is {locPrompt}. The provided image is structured such that the **left part represents Image 1** and the **right part represents Image 2**. 
    """
    
    CKLST_PROMPT = data["cklst-prompt"]
    EVAL_PROMPT = data["eval-prompt"]

    COMP_MAPPING = {
        "1 >> 2": 2,
        "1 > 2": 1,
        "1 = 2": 0,
        "1 < 2": -1,
        "1 << 2": -2,
        "2 >> 1": -2,
        "2 > 1": -1,
        "2 = 1": 0,
        "2 < 1": 1,
        "2 << 1": 2
    }

    def encodeAsPIL(imagelist, target_size=512,  as_base64=False):
        images = []
        total_width = 0
        max_height = 0

        for image_data in imagelist:
            img = image_data.convert("RGB")
            original_width, original_height = img.size

            new_height = target_size
            new_width = int((target_size / original_height) * original_width)
            img = img.resize((new_width, new_height))  # Resize the image
            images.append(img)

            total_width += new_width
            max_height = max(max_height, new_height)

        # Create a blank combined image
        combined_image = Image.new('RGB', (total_width + 5 * (len(imagelist) - 1), max_height))

        x_offset = 0
        for img in images:
            combined_image.paste(img, (x_offset, 0))
            x_offset += img.width + 5

            # If as_base64 is True, return Base64-encoded string
        if as_base64:
            img_byte_arr = io.BytesIO()
            combined_image.save(img_byte_arr, format='JPEG')
            img_byte_arr.seek(0)  # Move to the beginning of the BytesIO stream
            img_base64 = base64.b64encode(img_byte_arr.read()).decode('utf-8')
            return img_base64

        # Otherwise, return the PIL Image object
        return combined_image
    
    def validate_format(text):
        required_blocks = {
            'CHECKLIST': r'(\d+\.\s.+?:\s\d+%)',
            'COMPARISON': r'([^|]+\|[^|]+\|[^|]+\|(-1|0|1)\|\s?\d+%)',
            'ANALYSIS': r'\S+',  # At least some non-whitespace content
            'VERDICT': r'Final Verdict:\s*(Image1|Tie|Image2)'
        }
        
        try:
            # Block existence check
            for block in required_blocks:
                if not re.search(f'====BEGIN {block}====.*====END {block}====', text, re.DOTALL):
                    return False

            # Checklist validation
            checklist = re.findall(r'\d+\.\s(.+?):\s(\d+)%', 
                                re.search(r'====BEGIN CHECKLIST====(.*?)====END CHECKLIST====', text, re.DOTALL).group(1))
            if sum(int(w) for _,w in checklist) != 100 or len(checklist) < 2:
                return False

            # Comparison table validation
            comparison = re.search(r'====BEGIN COMPARISON====(.*?)====END COMPARISON====', text, re.DOTALL).group(1)
            lines = [line.strip() for line in comparison.split('\n') if line.strip()]
            if len(lines) < 2:
                return False
                
            # Verify comparison line format
            pattern = re.compile(r'^[^|]+\|[^|]+\|[^|]+\|(-1|0|1)\|\s?\d+%$')
            if not all(pattern.match(line) for line in lines[1:]):
                return False

            # Analysis content check
            analysis_content = re.search(r'====BEGIN ANALYSIS====(.*?)====END ANALYSIS====', text, re.DOTALL).group(1).strip()
            if len(analysis_content) < 50:  # Minimum analysis length
                return False

            return True
            
        except Exception as e:
            print(f"Validation error: {str(e)}")
            return False

    def parse_output(text):
        result = {
            'checklist': [],
            'comparison': [],
            'analysis': '',
            'verdict': None,
            'verdict_code': None
        }
        
        try:
            # Parse checklist
            checklist_match = re.search(r'====BEGIN CHECKLIST====(.*?)====END CHECKLIST====', text, re.DOTALL)
            if checklist_match:
                for m in re.finditer(r'\d+\.\s(.+?):\s(\d+)%', checklist_match.group(1)):
                    result['checklist'].append({
                        'dimension': m[1].strip(),
                        'importance': int(m[2])
                    })

            # Parse comparison table
            comparison_match = re.search(r'====BEGIN COMPARISON====(.*?)====END COMPARISON====', text, re.DOTALL)
            if comparison_match:
                lines = [line.strip() for line in comparison_match.group(1).split('\n') if line.strip()]
                header = lines[0].lower().split('|')
                for line in lines[1:]:
                    parts = [p.strip() for p in line.split('|')]
                    if len(parts) == 5:
                        result['comparison'].append({
                            'dimension': parts[0],
                            'image1': parts[1],
                            'image2': parts[2],
                            'preference': int(parts[3]),
                            'importance': int(parts[4].replace('%',''))
                        })

            # Parse analysis
            analysis_match = re.search(r'====BEGIN ANALYSIS====(.*?)====END ANALYSIS====', text, re.DOTALL)
            if analysis_match:
                result['analysis'] = analysis_match.group(1).strip()
            else:
                result['analysis'] = ""

            # Parse final verdict
            verdict_match = re.search(r'Final Verdict:\s*(Image1|Tie|Image2)', text)
            if verdict_match:
                verdict = verdict_match.group(1) 
                result['verdict'] = verdict
                if verdict == 'Image1':
                    result['verdict_code'] = 1
                elif verdict == 'Image2':
                    result['verdict_code'] = -1
                else:  # Tie
                    result['verdict_code'] = 0
            else:
                result['verdict'] = ""
                result['verdict_code'] = 0

            return result
            
        except Exception as e:
            print(f"Parsing error: {str(e)}")
            return None

    def verify_decision_integrity(parsed_data):
        """Ensure verdict aligns with analysis content"""
        if not parsed_data:
            return False
        
        # 2. Verify importance dimensions match
        checklist_dims = {item['dimension'] for item in parsed_data['checklist']}
        comparison_dims = {item['dimension'] for item in parsed_data['comparison']}
        if checklist_dims != comparison_dims:
            return False
            
        # 3. Validate analysis depth
        analysis_words = len(parsed_data['analysis'].split())
        if analysis_words < 100:  # Require substantial analysis
            return False
            
        return True


    def _fn(image1,image2, prompt="", Error=None, DEBUG=False, ANALYSIS=None, RECORD=False, cklth=3, toolbox=None):
        """
        打分
        """
        if toolbox is None:
            raise ValueError("pipe in Qwen")
        
        imagelist = [image1, image2]
        image = encodeAsPIL(imagelist)

        model, processor = toolbox

        msg = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                    },
                    {"type": "text", "text": FullPmt.format(locPrompt=prompt)},
                ],
            }
        ]


        # Preprocess the inputs
        text_prompt = processor.apply_chat_template(msg, add_generation_prompt=True)
        # Excepted output: '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\n<|im_start|>assistant\n'

        inputs = processor(
            text=[text_prompt], images=[image], padding=True, return_tensors="pt"
        )

        inputs = inputs.to("cuda")
        
        
        # 使用 generate 获取生成的序列
        generated_ids = model.generate(**inputs, max_new_tokens=1024, return_dict_in_generate=True, output_logits=True,)  # 输出 logit
        #额外加入 用来重新前向一遍

        # 获取生成的 token 和 logits
        generated_sequence = generated_ids.sequences 
        # generated_attention_mask = generated_sequence != pad_token_id
        # generated_position_ids = generated_attention_mask.cumsum(1) - generated_attention_mask.long()
        #print(f"[ok] b1")
        generated_output = model(
            input_ids=generated_sequence,
            return_dict=True,
            output_hidden_states=True,
            use_cache=False  # 不使用缓存，确保完整计算图
        )
        # generated_output = model(input_ids=generated_sequence, return_dict=True, output_hidden_states=True,use_cache=False)

        logits = generated_output["logits"]
        # 获取输入的长度（用于裁剪）
        input_length = inputs.input_ids.shape[1]

        # 裁剪 logits，仅保留生成部分的 logits (跳过输入部分的 logits)
        generated_logits = logits[:, :, :]  # (batch_size, generated_length, vocab_size)

        # 计算 logp (每个生成 token 的对数概率)
        logp_list = []
        for i, output_ids in enumerate(generated_sequence[:, input_length:]):
            # 获取每个生成 token 的 logits
            token_logits = generated_logits[i]

            # 对 logits 应用 softmax 转换为概率分布
            probs = torch.softmax(token_logits, dim=-1)

            # 提取生成 token 的概率
            token_probs = probs[range(len(output_ids)), output_ids]

            # 计算对数概率
            logp = torch.log(token_probs)
            logp_list.append(logp)

        # 将 logp 转化为张量 (batch_size, generated_length)
        logp_tensor = torch.stack(logp_list, dim=0)

        # 裁剪生成的 token，只保留生成部分
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_sequence)]

        total_logp = logp_tensor.mean(dim=1) 

        

        # 解码生成的文本
        ans = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]

        #breakpoint()

        

        fmt = validate_format(ans)

        res = parse_output(ans)

        RES_DICT = {
            "fomat correctness": fmt,
            "model chose": verdict_code,
            "lth":len(ans),
            "logp":total_logp,
        }

        #print(ans)
        #print(res)

        return RES_DICT
        
        
        
        # for principle, comparison in zip(principles, comparison_levels):
        #     try:
        #         Eval_Dict[principle.strip("* ")] = COMP_MAPPING[comparison.strip("* ")]
        #     except Exception:
        #         NumberOfWrongCompRES += 1
        #         WrongCompRES.append(comparison.strip("* "))

        # final_score = 0

        # for eva, score in Eval_Dict.items():
        #     final_score += score
        
        # isNULLANS = False
        # if len(Eval_Dict)==0:
        #     isNULLANS = True

        # if not isNULLANS:
        #     posANS, negANS = sum(k > 0 for k in Eval_Dict.values()), sum(k < 0 for k in Eval_Dict.values())
        #     posRATE, negRATE = posANS/len(Eval_Dict), negANS/len(Eval_Dict)
        # else:
        #     posANS, negANS, posRATE, negRATE = 0,0,0,0
        
        # if not principles:
        #     Error.write("\nRESPOND\n"+ EVAL)
        #     Error.flush()
        # elif not Eval_Dict:
        #     Error.write("\nRESPOND\n"+ EVAL)
        #     Error.flush()

        # ERROR_DICT = {
        #     "NumberOfWrongCompRES":NumberOfWrongCompRES,
        #     "WrongCompRES": WrongCompRES,
        #     "FIRST RESPOND TIME DELAY":cnt_1,
        #     "SECOND RESPOND TIME DELAY":cnt_2,
        #     "Is Null Answer":isNULLANS
        # }

        # ANS_DICT = {
        #     "Positive Rate":posRATE,
        #     "Negative Rate":negRATE,
        #     "Null Rate": 1 - posRATE - negRATE,
        #     "TOTAL": len(Eval_Dict),
        #     "Lenth":lth,
        # }

        # if DEBUG:
        #     print("[Score]")
        #     print(final_score)
        #     print("[DICT]")
        #     print(Eval_Dict)

        # if RECORD:
        #     ANALYSIS.write(ans2)
        #     ANALYSIS.flush()

        return final_score, ERROR_DICT, ANS_DICT
    
    return _fn



